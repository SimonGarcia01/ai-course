{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e668bac2-3adb-4da6-816b-a5a9b71a4cb1",
   "metadata": {},
   "source": [
    "### **XGBoost**\n",
    "XGBoost es un algoritmo basado en árboles de decisión, muy eficiente en la construcción de modelos de clasificación y regresión. Utilizaremos el conjunto de datos **Iris**, que contiene 150 ejemplos de flores de tres especies distintas. Cada ejemplo está descrito por cuatro características (longitud y ancho del sépalo y pétalo), y la tarea es clasificar las flores según su especie.\n",
    "\n",
    "Este es un ejemplo ilustrtivo para familiarizarnos con el uso de XGBoost. \n",
    "\n",
    "Para ejemplos consultar: https://github.com/dmlc/xgboost/tree/master/demo\n",
    "\n",
    "### 1. **Hiperparámetros principales en XGBoost**\n",
    "\n",
    "#### 1.1 **Tasa de aprendizaje (`learning_rate`)**\n",
    "- **Descripción**: Controla el tamaño del paso que da el modelo en cada iteración. Una tasa de aprendizaje más baja hace que el modelo avance más lentamente pero con mayor precisión.\n",
    "- **Rango típico**: [0.01, 0.3]\n",
    "- **Impacto**: Tasa baja requiere más árboles pero es menos probable que sobreajuste. Una tasa alta converge más rápido pero puede llevar a una solución subóptima.\n",
    "  \n",
    "#### 1.2 **Número de árboles (`n_estimators`)**\n",
    "- **Descripción**: Define el número de árboles que se entrenarán.\n",
    "- **Rango típico**: [50, 500]\n",
    "- **Impacto**: Un número mayor de árboles mejora el rendimiento si la tasa de aprendizaje es baja, pero también aumenta el riesgo de sobreajuste y el tiempo de entrenamiento.\n",
    "\n",
    "#### 1.3 **Máxima profundidad de los árboles (`max_depth`)**\n",
    "- **Descripción**: Establece la profundidad máxima de los árboles. Mayor profundidad permite capturar más patrones complejos pero aumenta el riesgo de sobreajuste.\n",
    "- **Rango típico**: [3, 10]\n",
    "- **Impacto**: Un valor bajo ayuda a reducir el sobreajuste, mientras que un valor alto mejora la capacidad predictiva para datos complejos.\n",
    "\n",
    "#### 1.4 **Mínimo peso de la hoja (`min_child_weight`)**\n",
    "- **Descripción**: Determina el peso mínimo de las hojas (número mínimo de instancias necesarias en una hoja). Controla la complejidad del árbol.\n",
    "- **Rango típico**: [1, 10]\n",
    "- **Impacto**: Valores bajos pueden conducir a un sobreajuste al permitir hojas muy pequeñas. Valores altos simplifican el modelo pero pueden perder información relevante.\n",
    "\n",
    "#### 1.5 **Fracción de columnas a muestrear (`colsample_bytree`)**\n",
    "- **Descripción**: Proporción de características que serán muestreadas para cada árbol.\n",
    "- **Rango típico**: [0.3, 1.0]\n",
    "- **Impacto**: Valores bajos ayudan a prevenir el sobreajuste y pueden mejorar la generalización. Valores altos utilizan más características por árbol y pueden capturar más patrones.\n",
    "\n",
    "#### 1.6 **Fracción de muestras a muestrear (`subsample`)**\n",
    "- **Descripción**: Porcentaje de muestras usadas para entrenar cada árbol.\n",
    "- **Rango típico**: [0.5, 1.0]\n",
    "- **Impacto**: Valores más bajos ayudan a prevenir el sobreajuste al introducir mayor aleatoriedad. Valores altos usan más muestras y generan árboles más completos.\n",
    "\n",
    "#### 1.7 **Gamma (`gamma`)**\n",
    "- **Descripción**: Controla si se deben realizar divisiones adicionales en los nodos de los árboles. Si el valor es mayor que cero, una división solo se realiza si el resultado mejora la métrica de pérdida.\n",
    "- **Rango típico**: [0, 5]\n",
    "- **Impacto**: Un valor alto requiere que una mejora significativa ocurra antes de realizar una división, lo que simplifica el modelo.\n",
    "\n",
    "### 2. **Estrategia para la Sintonización de Hiperparámetros**\n",
    "La sintonización de hiperparámetros puede realizarse utilizando técnicas como la **búsqueda en cuadrícula (Grid Search)** o la **búsqueda aleatoria (Random Search)**. Aquí utilizaremos **GridSearchCV** para realizar un ajuste sistemático.\n",
    "\n",
    "### 3. **Ejemplo Práctico con el Dataset Iris**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dde7c75-43d3-475c-8dd2-88bc8dc083ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 972 candidates, totalling 2916 fits\n",
      "Mejores hiperparámetros: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 50, 'subsample': 0.7}\n",
      "Precisión en el conjunto de prueba: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16079220\\AppData\\Local\\anaconda3\\envs\\bancow2\\lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar el conjunto de datos Iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configuración del clasificador XGBoost\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=3)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.7, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "\n",
    "# Predecir con los mejores hiperparámetros\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Calcular precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión en el conjunto de prueba:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c5e6f7-e3f3-43fc-9244-838ead250279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el conjunto de prueba: 1.0\n"
     ]
    }
   ],
   "source": [
    "#model = xgb.XGBClassifier(objective='multi:softmax', num_class=3)\n",
    "model = xgb.XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Calcular precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión en el conjunto de prueba:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803eb34-6433-4213-8b19-17636c20b42d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. **Resultados y Discusión:** Impacto de los hiperparámetros:\n",
    "\n",
    "- **Tasa de aprendizaje (`learning_rate`)**: Valores más bajos de `learning_rate` generalmente conducen a una mejor generalización, pero requieren más árboles. Si el modelo sobreajusta, reducir la tasa de aprendizaje puede ser una buena opción.\n",
    "  \n",
    "- **Número de árboles (`n_estimators`)**: Un mayor número de árboles suele mejorar la precisión, pero al costo de tiempos de entrenamiento más largos. Si tienes un modelo subentrenado (baja precisión), aumentar `n_estimators` puede mejorar el rendimiento.\n",
    "\n",
    "- **Máxima profundidad (`max_depth`)**: Este parámetro controla la complejidad del modelo. Un valor muy alto puede causar sobreajuste, especialmente en conjuntos de datos simples como Iris.\n",
    "\n",
    "- **Fracción de columnas (`colsample_bytree`) y fracción de muestras (`subsample`)**: Reducir estos valores puede prevenir el sobreajuste, especialmente cuando tienes muchas características o datos limitados. Introduce más aleatoriedad y hace que los modelos sean más robustos.\n",
    "\n",
    "- **`Gamma` y `min_child_weight`**: Ayudan a controlar la complejidad del árbol. Valores más altos tienden a simplificar el modelo, mientras que valores más bajos permiten captar más patrones pero pueden llevar al sobreajuste.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a8714-8710-415d-a50d-94ee4a486f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaea8f8-ab07-4d72-9959-cdf53e84b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar conjunto de datos MNIST\n",
    "mnist = fetch_openml('mnist_784',parser='auto')\n",
    "X = mnist.data / 255.0  # Normalize pixel values to 0-1 range\n",
    "y = np.array(mnist.target).astype(int)\n",
    "images =  np.zeros((28,1))\n",
    "for i in range(10):\n",
    "    image =np.array(X.iloc[i]).reshape((28,28))\n",
    "    images=np.append(images,image,axis=1)\n",
    "print(\"\\n\\n10 primeras imagenes de prueba\")    \n",
    "plt.imshow(images,cmap=\"gray\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee857da-4d87-4778-a1b6-23ba368c7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "num_class = np.unique(y).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66125362-87a2-4b3a-aa66-f738f7aa5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_class, n_estimators=2)\n",
    "\n",
    "#model = xgb.XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión en el conjunto de prueba:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
